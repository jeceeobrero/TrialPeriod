# Backend API

This Backend API displays and parses+summarizes three reputable sources namely, BBC, The Guardian, and Daily Mail. It uses Beautiful Soup for web scraping, GPT-3 for Summarizing and an REST API. This Backend uses Postgresql as its database.


## Methods
1) GET all news articles
```
    # Retrieve all NewsArticle objects from the database
    news_articles = NewsArticle.objects.all()

    # Serialize the NewsArticle objects
    serializer = NewsArticleSerializer(news_articles, many=True)

    # Return the serialized NewsArticle objects as a JSON response
    return Response({"news_articles": serializer.data})
```
2) POST all parsed and summarized articles. 
    2.1) The flow of the POST request is to get the parsed articles.
        ```
            news_articles = parse_bbc()
        ```
    2.2) Iterate each one to get their article details.
        ````
            try:
                for article in news_articles[10:]:
                    # Extract all article details
                    title = article.get('Title')
                    author = article.get('Author')
                    published_date = article.get('Published Date')
                    parsed_content = article.get('Content')
        `````
    2.3) Check if it is already in the cache meaning it is already parsed, summarized, and added to database, then no need to generate summary and add to database. Else, we have to generate their summary, add to database, and lastly, add it to cache so that next time, it will not go through the same process again. This will save time and resources. The cache_key will serve as the one that we will look for.
        ```
            url = article.get('URL')
            cache_key = f'news_article_{url}'
            news_article = cache.get(cache_key)

            # If not yet in cache, generate its summary and add to DB. Else, no need to parse and summarize it again.
            if news_article is None:
                # Summarize the parsed content using your machine learning script
                summarized_content = generate_summary(article)

                # Create a NewsArticle object and save it to the database
                news_article = NewsArticle(
                    url=url,
                    title=title,
                    author=author,
                    published_date=published_date,
                    parsed_content=parsed_content,
                    summarized_content=summarized_content,
                )
                news_article.save()

                # Cache the news article for future requests
                cache.set(cache_key, news_article)
        ```
    2.4) If it successfully renders, it will have a response of "Success". Else, "Error".
        ```
                return Response("Success")
            except:
                return Response("Error")
        ```

- The following codes serve as the backend of News Article app where it has an API that has a GET Method for getting the list of news articles and a POST Method to add the parsed and summarized news articles from the chosen news sources. 

- This is a Django Rest Framework and this is the API app folder. The following are the files and its contents.

- Folder is admin.py
- Contents are:
from django.contrib import admin
from .models import NewsArticle
# Register your models here.

admin.site.register(NewsArticle)

- Folder is apps.py
- Contents are:
from django.apps import AppConfig

class ApiConfig(AppConfig):
    name = 'api'

- Folder is models.py (this serves as the model of the News Article)
- Contents are:
from django.db import models

class NewsArticle(models.Model):
    url = models.CharField(max_length=255)
    title = models.CharField(max_length=100)
    author = models.CharField(max_length=255)
    published_date = models.DateTimeField()
    parsed_content = models.TextField()
    summarized_content = models.TextField(null=True)

    def __str__(self):
        return self.title

- Folder is serializers.py (This is helpful in passing the needed data to the frontend and vice versa)
- Contents are:
from rest_framework import serializers
from .models import NewsArticle

class NewsArticleSerializer(serializers.ModelSerializer):
    class Meta:
        model = NewsArticle
        fields = '__all__'

- Folder is urls.py (This serves as the paths in navigating to the News Article List View)
- Contents are:
from django.urls import path
from .views import NewsArticleList

urlpatterns = [
    path('articles/', NewsArticleList.as_view(), name='article-list')
]

- Folder is views.py (This serves as the logic or implementation in dealing with the data and this contains the GET and POST methods for the list of articles)
- Contents are:
from django.core.cache import cache
from .models import NewsArticle
from .serializers import NewsArticleSerializer
from rest_framework.views import APIView
from rest_framework.response import Response
from scraper.scraper import parse_bbc, parse_guardian, parse_daily_mail
from summarizer.summarizer import generate_summary
from django.db import transaction


class NewsArticleList(APIView):
    def get(self, request):
        # Retrieve all NewsArticle objects from the database
        news_articles = NewsArticle.objects.all()

        # Serialize the NewsArticle objects
        serializer = NewsArticleSerializer(news_articles, many=True)

        # Return the serialized NewsArticle objects as a JSON response
        return Response({"news_articles": serializer.data})

    @transaction.atomic
    def post(self, request):
        # Get the parsed articles
        news_articles = parse_bbc()

        try:
            for article in news_articles[10:]:
                # Extract all article details
                title = article.get('Title')
                author = article.get('Author')
                published_date = article.get('Published Date')
                parsed_content = article.get('Content')

                # Check if the news article has already been scraped
                url = article.get('URL')
                cache_key = f'news_article_{url}'
                news_article = cache.get(cache_key)

                # If not yet in cache, generate its summary and add to DB. Else, no need to parse and summarize it again.
                if news_article is None:
                    # Summarize the parsed content using your machine learning script
                    summarized_content = generate_summary(article)

                    # Create a NewsArticle object and save it to the database
                    news_article = NewsArticle(
                        url=url,
                        title=title,
                        author=author,
                        published_date=published_date,
                        parsed_content=parsed_content,
                        summarized_content=summarized_content,
                    )
                    news_article.save()

                    # Cache the news article for future requests
                    cache.set(cache_key, news_article)
                    
            # Return the serialized NewsArticle object as a JSON response
            return Response("Success")
        except:
            return Response("Error")
